{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Customer Churn \u2013 Data Cleaning Workflow\n", "\n", "This notebook documents the full data cleaning workflow for the **Customer Subscription & Churn** dataset.\n", "\n", "It assumes you have a raw file named `customer_churn_raw.xlsx` in the same folder.\n", "\n", "Main steps:\n", "- Load the raw dataset\n", "- Inspect structure and data quality issues\n", "- Standardize column names\n", "- Clean string fields (country, plan type, etc.)\n", "- Parse and normalize dates (signup and last active)\n", "- Convert numeric fields from messy strings to numbers\n", "- Handle missing values\n", "- Remove duplicates\n", "- Basic handling of outliers in `monthly_spend`\n", "- Export a clean, analysis-ready file as `customer_churn_clean.xlsx`\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "\n", "pd.set_option('display.max_columns', None)\n", "pd.set_option('display.width', 140)\n", "\n", "print('pandas version:', pd.__version__)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Load raw data\n", "\n", "The raw data file should be named **`customer_churn_raw.xlsx`** and be placed in the same directory as this notebook."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["raw_path = 'customer_churn_raw.xlsx'  # update if your file name is different\n", "df_raw = pd.read_excel(raw_path)\n", "\n", "df_raw.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_raw.info()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["From the initial inspection, you may see:\n", "- Inconsistent column naming (spaces, casing)\n", "- `Signup Date` and `Last Active` as strings with mixed formats\n", "- Numeric columns like `Age`, `Monthly Spend`, `Num Logins` stored as `object` due to messy formatting\n", "- Text columns (country, plan type) with trailing spaces and inconsistent casing\n", "- Churn status as 0/1 with messy formatting in some rows"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Create working copy and standardize column names\n", "\n", "We avoid modifying the raw DataFrame directly and work on a copy. Column names are standardized to:\n", "- strip whitespace\n", "- convert to lowercase\n", "- replace spaces with underscores\n", "- remove problematic characters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = df_raw.copy()\n", "\n", "df.columns = (\n", "    df.columns\n", "      .str.strip()\n", "      .str.lower()\n", "      .str.replace(r'\\s+', '_', regex=True)\n", "      .str.replace('[()]', '', regex=True)\n", ")\n", "\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Typical standardized column names after this step:\n", "\n", "- `customer_id`\n", "- `signup_date`\n", "- `last_active`\n", "- `country`\n", "- `age`\n", "- `monthly_spend`\n", "- `num_logins`\n", "- `churned`\n", "- `plan_type`\n", "- `satisfaction_score`\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Clean string columns (country, plan_type, etc.)\n", "\n", "We:\n", "- Strip leading/trailing spaces\n", "- Replace empty strings / placeholders (`'?', 'NA', ''`) with proper `NaN`\n", "- Normalize casing (e.g. `\"Singapore \"` \u2192 `\"SINGAPORE\"`)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Clean all object columns first\n", "for col in df.select_dtypes(include='object').columns:\n", "    df[col] = df[col].astype(str).str.strip()\n", "    df[col] = df[col].replace({'': np.nan, 'na': np.nan, 'NA': np.nan, '?': np.nan})\n", "\n", "# Normalize key categorical columns\n", "cat_cols = ['country', 'plan_type']\n", "for col in cat_cols:\n", "    if col in df.columns:\n", "        df[col] = df[col].str.upper()\n", "\n", "df[cat_cols].head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can additionally map values like `\"MALAYSIA\"`, `\"SINGAPORE\"`, etc. to standardized forms,\n", "or group rare categories as needed for modeling."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Convert dates and numeric fields\n", "\n", "The `signup_date` and `last_active` columns may have different date formats. We convert both to datetime.\n", "\n", "Numeric columns such as `age`, `monthly_spend`, `num_logins`, `churned`, and `satisfaction_score` may contain\n", "messy strings or currency-looking values, so we remove non-numeric characters and convert to numeric types."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["date_cols = ['signup_date', 'last_active']\n", "for col in date_cols:\n", "    if col in df.columns:\n", "        df[col] = pd.to_datetime(df[col], errors='coerce')\n", "\n", "numeric_cols = ['age', 'monthly_spend', 'num_logins', 'churned', 'satisfaction_score']\n", "for col in numeric_cols:\n", "    if col in df.columns:\n", "        df[col] = (\n", "            df[col]\n", "              .astype(str)\n", "              .str.replace(r'[^0-9\\.-]', '', regex=True)\n", "        )\n", "        df[col] = pd.to_numeric(df[col], errors='coerce')\n", "\n", "df[numeric_cols].head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.info()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Handle missing values\n", "\n", "Strategy used here:\n", "- Rows missing critical identifiers (e.g. `customer_id`) are dropped\n", "- Numeric fields with missing values are filled with their median\n", "- Date fields that are critical for analysis may be dropped or left as `NaT`, depending on the use case"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Drop rows with missing critical ID\n", "if 'customer_id' in df.columns:\n", "    df = df.dropna(subset=['customer_id'])\n", "\n", "# Impute numeric columns with median\n", "for col in numeric_cols:\n", "    if col in df.columns:\n", "        median_val = df[col].median()\n", "        df[col] = df[col].fillna(median_val)\n", "\n", "df.isna().sum()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You may choose stricter rules for certain analysis use cases (e.g. dropping rows with missing `signup_date`)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6. Remove duplicates\n", "\n", "We remove exact duplicate rows to avoid double counting customer records."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["before_rows = len(df)\n", "df = df.drop_duplicates()\n", "after_rows = len(df)\n", "print(f'Removed {before_rows - after_rows} duplicate rows')\n", "\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7. Basic outlier handling for `monthly_spend`\n", "\n", "The dataset may include extreme outliers in `monthly_spend` (e.g. values 10\u201315\u00d7 higher than typical).\n", "We use an IQR-based cap to prevent these from dominating the analysis while retaining the records."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if 'monthly_spend' in df.columns:\n", "    q1 = df['monthly_spend'].quantile(0.25)\n", "    q3 = df['monthly_spend'].quantile(0.75)\n", "    iqr = q3 - q1\n", "    upper_cap = q3 + 1.5 * iqr\n", "    print('monthly_spend upper cap (IQR-based):', upper_cap)\n", "\n", "    df['monthly_spend_capped'] = df['monthly_spend'].clip(upper=upper_cap)\n", "\n", "df[['monthly_spend', 'monthly_spend_capped']].head() if 'monthly_spend' in df.columns else df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As with the retail project, you can decide whether to keep both original and capped versions depending on modeling strategy."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8. Final checks\n", "\n", "We run a last round of checks to confirm types, ranges, and summary statistics."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.info()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.describe(include='all').transpose().head(20)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 9. Export cleaned dataset\n", "\n", "Finally, we export the cleaned dataset to an Excel file named **`customer_churn_clean.xlsx`**.\n", "This file is suitable for churn modeling, dashboards, and retention analytics."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clean_path = 'customer_churn_clean.xlsx'\n", "df.to_excel(clean_path, index=False)\n", "print('Cleaned dataset saved to:', clean_path)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "### Notes for Upwork / Portfolio\n", "\n", "- This notebook can be attached in your GitHub repository as a complete example of CRM/churn data cleaning.\n", "- It demonstrates handling of messy dates, corrupted numeric fields, categorical standardization, missing data, duplicates, and outliers.\n", "- You can adapt it to real client datasets by updating the column names and file paths."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}